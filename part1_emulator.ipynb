{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load essential and useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data, inspect and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following indices grab our model parameters from `Walkers`\n",
    "inds = [1,2,3,4,5,6,10,11,14]\n",
    "\n",
    "# define the meaning for the features, i.e. your model parameters\n",
    "parameters = np.array([r\"$\\log_{10} f_{*,10}$\",\n",
    "                       r\"$\\alpha_*$\",\n",
    "                       r\"$\\log_{10} f_{\\rm esc, 10}$\",\n",
    "                       r\"$\\alpha_{\\rm esc}$\",\n",
    "                       r\"$\\log_{10}[M_{\\rm turn}/{\\rm M}_{\\odot}]$\",\n",
    "                       r\"$t_*$\",\n",
    "                       r\"$\\log_{10}\\frac{L_{\\rm X<2keV}/{\\rm SFR}}{{\\rm erg\\ s^{-1}\\ M_{\\odot}^{-1}\\ yr}}$\",\n",
    "                       r\"$E_0/{\\rm keV}$\",\n",
    "                       r\"$\\alpha_{\\rm X}$\"])\n",
    "                       \n",
    "# and their limits\n",
    "limits = np.array([[-3,0], [-0.5,1], [-3,0],[-1,0.5], [8,10], [0.01,1], [38,42], [0.1,1.5], [-1,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note again, the entire ~0.5M database was built through a Bayesian inference run. Therefore we do have a posterior to compare with after having an emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiNest's output contains normalized parameters by mapping points in the range of `limits` to [0,1]. The last column gives lnL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's rescaled them back and visualize the distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore: We will be training using the ~450k points shown before that more or less follows the posterior. However, does it matter to train the network using points from a posterior or from a uniform distribution like what people normally do when they do not have a posterior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's just visualize what the prediction looks like using the MAP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "points in AllFile3.h5 are in order, i.e. the nth row in `Walkers` corresponds to the nth row in other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('AllFile3.h5','r') as f:\n",
    "    zs = f['AveDatas'][ML_index,:,0] # redshift for global signals; this is the same for all models by construction\n",
    "    xH = f['AveDatas'][ML_index,:,1] # neutral fraction\n",
    "    Tb = f['AveDatas'][ML_index,:,2] # 21cm brightness temperature\n",
    "    tau_e = f['TauDatas'][ML_index]  # CMB optical depth\n",
    "    \n",
    "fig, (axxH, axTb) = plt.subplots(2,1, figsize=(15,8), sharex=True)\n",
    "\n",
    "# model\n",
    "...\n",
    "\n",
    "# observations\n",
    "# Dark Pixels\n",
    "axxH.errorbar([5.6,6.07], [0.04,0.38], yerr=[[0,0],[0.05,0.20]], fmt='o',color='k', label='McGreer+15',mfc='white',capsize=5, elinewidth=2, markeredgewidth=1,alpha=1)\n",
    "axxH.errorbar([5.6,6.07], [0.04,0.38], yerr=[[0.03,0.1],[0,0]], mfc='white',uplims=True, fmt=' ',color='k',alpha=1)\n",
    "axxH.errorbar([5.9], [0.06], yerr=[[0],[0.05]],mfc='white', fmt='o',color='k',capsize=5, elinewidth=2, markeredgewidth=1,alpha=1)\n",
    "axxH.errorbar([5.9], [0.06], yerr=[[0.03],[0]], uplims=True, fmt=' ',color='k',alpha=1)\n",
    "axxH.errorbar([5.61,5.8,5.99,6.21,6.35], [0.42, 0.53,0.67,0.53,0.69], yerr=[[0,0,0,0,0],[0.05,0.07,0.07,0.11,0.15]], markersize=10,fmt='o',color='k', label='Campo+in prep.',capsize=5, elinewidth=2, markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([5.61,5.8,5.99,6.21,6.35], [0.42, 0.53,0.67,0.53,0.69], yerr=[[0.1,0.1,0.1,0.1,0.1],[0,0,0,0,0]], markersize=20,uplims=True, fmt=' ',color='k',alpha=1)\n",
    "\n",
    "# QSO damping\n",
    "axxH.errorbar([7.0], [0.70], yerr=[[0.23],[0.20]], fmt='p',color='b', label='Wang+20',markersize=8,capsize=5, elinewidth=2,mfc='white', markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([7.5413], [0.56], yerr=[[0.18],[0.21]], fmt='h',color='b', label='BaÃ±ados+18',markersize=8,capsize=5, elinewidth=2,mfc='white', markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([7.0851,7.5413], [0.40,0.21], yerr=[[0.19,0.19],[0.21,0.17]], fmt='*',color='b', label='Greig+17/19',markersize=10,capsize=5, elinewidth=2, mfc='white',markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([7.0851,7.5413], [0.48,0.60], yerr=[[0.26,0.23],[0.26,0.20]], fmt='s',color='b', label='Davies+18',markersize=8,capsize=5, elinewidth=2,mfc='white', markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([7.29], [0.49], yerr=[0.11], xerr=[0.20], fmt='8',color='b', label='Greig+in prep.',markersize=20,capsize=5, elinewidth=2, markeredgewidth=2,alpha=1)\n",
    "\n",
    "# LAE fraction\n",
    "## LF\n",
    "axxH.errorbar([6.9], [0.33], yerr=[[0.1],[0.]], uplims=True, fmt='<',color='purple', label='Wold+21',markersize=8,capsize=5, elinewidth=2,  mfc='white',markeredgewidth=2,alpha=1)\n",
    "axxH.errorbar([7.3], [0.5], yerr=[[0.3],[0.1]], fmt='>',color='purple', label='Inoue+18',markersize=8,capsize=5, elinewidth=2,  mfc='white',markeredgewidth=2,alpha=1)\n",
    "#axxH.errorbar([5.7,6.6,7.0], [0.4,0.4,0.4], yerr=[[0.1,0.1,0.1],[0,0,0]], uplims=True, fmt='s',color='black',alpha=0.6)\n",
    "axxH.errorbar([6.6,7.0,7.3], [0.08,0.28,0.83], yerr=[[0.05,0.05,0.07],[0.08,0.05,0.06]], fmt='^',color='purple', label='Morales+21',markersize=8,capsize=5, elinewidth=2,  mfc='white',markeredgewidth=2,alpha=1)\n",
    "## clustering\n",
    "axxH.errorbar([6.6], [0.15], yerr=[0.15], fmt='v',color='purple', label='Ouchi+18',markersize=10, capsize=5,mfc='white', elinewidth=2, markeredgewidth=2,alpha=1)\n",
    "\n",
    "## EW\n",
    "axxH.errorbar([7.0], [0.55], yerr=[[0.13],[0.11]], fmt='v',color='g', label='Whitler+20',markersize=10,capsize=5, elinewidth=2, mfc='white',markeredgewidth=2,alpha=1) #EW\n",
    "axxH.errorbar([7.9], [0.76], xerr=[0.6], yerr=[[0.],[0.1]], lolims=True,fmt='<',label='Mason+19',color='g', markersize=10,capsize=5, elinewidth=2, mfc='white',markeredgewidth=2,alpha=1) #EW\n",
    "axxH.errorbar([7.6], [0.88], yerr=[[0.1],[0.05]], xerr=[0.6],fmt='>',color='g', label='Hoag+19',markersize=10, capsize=5,mfc='white', elinewidth=2, markeredgewidth=2,alpha=1) #EW\n",
    "axxH.errorbar([7.6], [0.36], yerr=[[0.14],[0.10]], fmt='^',color='g', label='Jung+21',markersize=10, capsize=5,mfc='white', elinewidth=2, markeredgewidth=2,alpha=1)\n",
    "\n",
    "# CMB optical depth\n",
    "axxH.text(0.98, 0.95, r'${\\rm Planck18:} \\tau_e = 0.0569^{+0.0081}_{-0.0066}$'+'\\n'+\n",
    "          'MAP: %.4f'%tau_e,ha='right',va='top',fontsize = 15,transform=axxH.transAxes) \n",
    "\n",
    "\n",
    "## EDGES\n",
    "axTb.axvspan(15,20, alpha=0.3, color='k')\n",
    "axTb.text(15, 15, 'EDGES (Bowman+18)',ha='left',va='top',fontsize = 15) \n",
    "\n",
    "axxH.legend(loc='lower right',fontsize=12,ncol=3,frameon=False)\n",
    "axxH.set_ylabel(r\"$\\bar{x}_{\\rm HI}$\",fontsize=15)\n",
    "axTb.set_ylabel(r\"$\\bar{T}_{21}/{\\rm mK}$\",fontsize=15)\n",
    "axTb.set_xlabel('redshift', fontsize=15)\n",
    "axTb.set_xlim(5,30)\n",
    "axTb.set_ylim(-110,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21cm power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('AllFile3.h5','r') as f:\n",
    "    ks = f['TotalPSDatas'][ML_index,:,0] # wavenumber for 21-cm power spectra, this is the same for all models by construction \n",
    "    ps = f['TotalPSDatas'][ML_index,:,1:]\n",
    "    \n",
    "# define the redshifts to plot\n",
    "snapshots = [61,56,51,47,39,32,25,19,13,8, 4,0]\n",
    "\n",
    "# define the axes limits\n",
    "...\n",
    "\n",
    "fig, axsPS = plt.subplots(3,4, figsize= (15,9), sharex=True, sharey=True)\n",
    "axsPS = axsPS.flatten()\n",
    "\n",
    "# the MAP model\n",
    "...\n",
    "\n",
    "# observations\n",
    "PS_limit_ks_z = np.fromfile('HERA_Phase1_Limits/PS_limit_ks_z8.bin') \n",
    "PS_limit_vals_z = np.fromfile('HERA_Phase1_Limits/PS_limit_vals_z8.bin')\n",
    "PS_limit_vars_z = np.fromfile('HERA_Phase1_Limits/PS_limit_vars_z8.bin')\n",
    "axsPS[8].errorbar(PS_limit_ks_z, PS_limit_vals_z, c='black', ls='', marker='s', ms=10, yerr=[np.zeros_like(PS_limit_vars_z), PS_limit_vars_z**0.5], alpha=0.7)\n",
    "axsPS[8].errorbar(PS_limit_ks_z, PS_limit_vals_z, c='black', ls='', marker='s', ms=10, yerr=[PS_limit_vals_z*0.5, np.zeros_like(PS_limit_vars_z)], alpha=0.7, uplims=True)\n",
    "PS_limit_ks_z = np.fromfile('HERA_Phase1_Limits/PS_limit_ks_z10.bin')\n",
    "PS_limit_vals_z = np.fromfile('HERA_Phase1_Limits/PS_limit_vals_z10.bin')\n",
    "PS_limit_vars_z = np.fromfile('HERA_Phase1_Limits/PS_limit_vars_z10.bin')\n",
    "axsPS[6].errorbar(PS_limit_ks_z, PS_limit_vals_z, c='black', ls='', marker='s', ms=10, yerr=[np.zeros_like(PS_limit_vars_z), PS_limit_vars_z**0.5], alpha=0.7)\n",
    "axsPS[6].errorbar(PS_limit_ks_z, PS_limit_vals_z, c='black', ls='', marker='s', ms=10, yerr=[PS_limit_vals_z*0.5, np.zeros_like(PS_limit_vars_z)], alpha=0.7, uplims=True)\n",
    "\n",
    "# cosmetics\n",
    "for ii in range((len(snapshots))):\n",
    "    axsPS[ii].text(0.01, 0.99, r'$z=%.1f$'%zs[::-1][snapshots[ii]],horizontalalignment='left',verticalalignment='top',\n",
    "                    transform=axsPS[ii].transAxes,fontsize = 15) \n",
    "    axsPS[ii].axvspan(xlim[0], 0.1, color='#e5c494',hatch='x', alpha=1)\n",
    "    axsPS[ii].axvspan(1, xlim[1], color='#e5c494',hatch='x', alpha=1)\n",
    "\n",
    "    axsPS[ii].grid(False)\n",
    "    axsPS[ii].set_xscale('log')\n",
    "    axsPS[ii].set_yscale('log')\n",
    "    axsPS[ii].set_xlim(xlim)\n",
    "    axsPS[ii].set_ylim(ylim)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05,wspace=0.05)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "\n",
    "plt.xlabel('\\n\\n'+r'$k[{\\rm Mpc}^{-1}]$', fontsize=15)\n",
    "plt.ylabel(r'$\\Delta_{21}^2[{\\rm mK}^{2}]$'+'\\n\\n', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### galaxy luminosity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined LF redshifts and UV magnitudes\n",
    "LF_redshifts = [6,7,8,10,12,15]\n",
    "Muvs = np.linspace(-30,-5,100)\n",
    "fig, axsLF = plt.subplots(2,3, figsize=(15,8), sharex=True, sharey=True)\n",
    "axsLF = axsLF.flatten()\n",
    "\n",
    "with h5py.File('AllFile3.h5','r') as f:\n",
    "    for qq, redshift in enumerate(LF_redshifts):\n",
    "        \n",
    "        ...\n",
    "        # varing models output the number density at different UV magnitudes.\n",
    "        # to eliminate having UV magnitudes also as an output, \n",
    "        # we use interpolation to force all models to output number density at the same magnitudes, i.e., `Muvs`.\n",
    "        ...\n",
    "        \n",
    "        axsLF[qq].plot(...,..., color='r',alpha=0.7, lw=5)\n",
    "    \n",
    "        fLF = 'LFs/LF_obs_Bouwens_%.6f.txt'%redshift\n",
    "        if os.path.exists(fLF):\n",
    "            datainput = np.loadtxt(fLF)\n",
    "            axsLF[qq].errorbar(datainput[:,0], (datainput[:,1]),yerr=datainput[:,2], fmt='s',color='black',zorder=2)\n",
    "        axsLF[qq].text(0.95,0.98, \"z=%d\"%redshift,horizontalalignment='right',\\\n",
    "                      verticalalignment='top',transform=axsLF[qq].transAxes,fontsize=15)\n",
    "\n",
    "        axsLF[qq].set_xlim(-8,-22)\n",
    "        axsLF[qq].set_ylim(2e-5,10)\n",
    "        axsLF[qq].set_yscale('log')\n",
    "        axsLF[qq].axvspan(-20, -22, color='#e5c494',hatch='x', alpha=1)\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05,wspace=0.05)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "\n",
    "plt.xlabel('\\n\\n'+r'$M_{\\rm 1500}$', fontsize=15)\n",
    "plt.ylabel(r'$\\phi$'+'\\n\\n', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Having the database inspected and understood, we now begin to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a few more data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have defined the features of our network. Let's normalize from having a range of `limits` to [0,1] to elimate having different dynamic ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ( features - limits[:,0] ) / (limits[:,1] - limits[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick the outputs that we would like to emulate with the network. \n",
    "\n",
    "For the purpose of using emulator for quick Bayesian inference, we emulate points that will go into the likelihoods. \n",
    "\n",
    "These are xHI at z=5.9 (vs McGreer+15, 1D); \n",
    "\n",
    "CMB optical depth (vs Planck+18, 1D); \n",
    "\n",
    "21-cm PS at z=8 (19D) and z=10 (18D); and \n",
    "\n",
    "UV LFs at z=6 (15D); let's ignore z=7(5D), 8(4D) and 10(3D) for speed.\n",
    "\n",
    "In total, the outputs are 54D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('database.npy'):\n",
    "    outputs = np.load('database.npy')\n",
    "else:\n",
    "    outputs = np.zeros([len(features), 54])\n",
    "\n",
    "    current_index = 0\n",
    "    with h5py.File('AllFile3.h5','r') as f:\n",
    "\n",
    "        # neutral fraction\n",
    "        outputs[:,current_index] = f['AveDatas'][:,-1,1] # by design, the last entry is for z=5.9\n",
    "        current_index+=1\n",
    "\n",
    "        # CMB optical depth\n",
    "        outputs[:,current_index] = ...\n",
    "        current_index...\n",
    "\n",
    "        #z=8 21cm PS, still need to interpolate to get the power at the observed wavenumbers\n",
    "        PS_limit_ks_z = np.fromfile('HERA_Phase1_Limits/PS_limit_ks_z8.bin') \n",
    "        ps = np.log10(f['TotalPSDatas'][:,:,1+snapshots[8]]) # zs[::-1][snapshots[8]] = 8\n",
    "        # network normally cannot deal with NaN/inf, the simplest way is to replace them with zeros. \n",
    "        # You can also mask those points out or resample those invalid points from some distribution.\n",
    "        ps = np.nan_to_num(ps) \n",
    "        for ii in range(len(features)):\n",
    "            outputs[ii, current_index:current_index+len(PS_limit_ks_z)] = interp1d(ks, ps[ii], fill_value=\"extrapolate\")(PS_limit_ks_z)\n",
    "        current_index+=len(PS_limit_ks_z)\n",
    "\n",
    "        #z=10 21cm PS, still need to interpolate to get the power at the observed wavenumbers\n",
    "        PS_limit_ks_z = ...\n",
    "        ps = ...\n",
    "        ps = ...\n",
    "        for ii in range(len(features)):\n",
    "            outputs[ii, current_index:current_index+len(PS_limit_ks_z)] = ...\n",
    "        current_index+=...\n",
    "\n",
    "        # UV LF, still need to interpolate to get the number density at the observed UV magnitudes\n",
    "        redshift=6\n",
    "        fLF = 'LFs/LF_obs_Bouwens_%.6f.txt'%redshift\n",
    "        observed_muv = np.loadtxt(fLF, usecols=0)\n",
    "        observed_muv = observed_muv[observed_muv>-20]\n",
    "\n",
    "        results = f['LFDatas_%d'%redshift]\n",
    "        for ii in range(len(features)):\n",
    "            outputs[ii, current_index:current_index+len(observed_muv)] = ...\n",
    "\n",
    "        current_index+=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training set, validation set, test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = ...\n",
    "f_valid = ...\n",
    "f_test = 1 - f_valid - f_train\n",
    "\n",
    "...\n",
    "\n",
    "N_train = \n",
    "N_valid =  \n",
    "N_test = ...\n",
    "\n",
    "print('Training set size: ', N_train)\n",
    "print('Validation set size: ', N_valid)\n",
    "print('Test set size: ', N_test)\n",
    "\n",
    "X_train = features[:N_train]\n",
    "X_valid = features[N_train:N_train+N_valid]\n",
    "X_test = features[-N_test:]\n",
    "\n",
    "Y_train = outputs[:N_train]\n",
    "Y_valid = outputs[N_train:N_train+N_valid]\n",
    "Y_test = outputs[-N_test:]\n",
    "\n",
    "\n",
    "# This is the size of a single training batch (training is not done on the entire test set at once, but on batches of data)\n",
    "# You can play with this number but it shouldn't make a very big difference as long as it is ~ 100 \n",
    "# If it is too big / small, the network will not learn as well\n",
    "batch_size = 64 \n",
    "\n",
    "# convert the database into tensorflow format\n",
    "x_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "x_val = tf.data.Dataset.from_tensor_slices(X_valid)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(Y_train)\n",
    "y_val = tf.data.Dataset.from_tensor_slices(Y_valid)\n",
    "training_data = tf.data.Dataset.zip((x_train, y_train)).shuffle(X_train.shape[0]).batch(batch_size)\n",
    "validation_data = tf.data.Dataset.zip((x_val, y_val)).shuffle(X_valid.shape[0]).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some callback functions to improve the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks..., # If loss does not improve for 20 epochs, reduce learning rate\n",
    "    tf.keras.callbacks... # If loss does not improve for 50 epochs, stop the learning\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size = number of params\n",
    "input_layer = ...\n",
    "\n",
    "# Hidden fully-connected layers\n",
    "# Number of nodes and hidden layers is arbitrary\n",
    "output = ...\n",
    "# Batch normalization = normalize the training weights at every batch.\n",
    "# You can try removing it and you will find that the results are a bit worse\n",
    "# This is because normalization helps stabilising the network. It can also help it run faster.\n",
    "# In general, batch normalization always helps improve accuracy a little bit.\n",
    "output = ...\n",
    "...\n",
    "# Last layer output shape = number of redshift bins in globale signal = 84\n",
    "output = ...\n",
    "model = ...\n",
    "model.summary()\n",
    "\n",
    "# define optimizer\n",
    "opt = ...\n",
    "# Use mean squared error for the loss function\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check how the traning goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, figsize=(10,8), sharex=True)\n",
    "\n",
    "axs[0].plot(...)\n",
    "axs[0].plot(...)\n",
    "axs[0].set_ylabel('MSE Loss', fontsize=15)\n",
    "axs[0].legend(loc='upper right')\n",
    "axs[1].plot(...)\n",
    "axs[1].set_ylabel('Learning Rate', fontsize=15)\n",
    "axs[1].set_xlabel('Epoch', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05,wspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test emulator with test set = data that the network never saw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's first take a fewer random test set and see the 21cm PS and galaxy UV LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest_show = 5\n",
    "current_index = 2 # skip the first two 1D datasets\n",
    "colors         = ['#984ea3','#ff7f00','#fec44f','#a65628','#f781bf']\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(10,8), sharex=True)\n",
    "for ii in range(Ntest_show):\n",
    "     \n",
    "    #z=8 21cm PS\n",
    "    PS_limit_ks_z = np.fromfile('HERA_Phase1_Limits/PS_limit_ks_z8.bin')\n",
    "    axs[0].plot(PS_limit_ks_z, ..., color=colors[ii])\n",
    "    axs[0].plot(PS_limit_ks_z, ..., color=colors[ii], ls='-.')\n",
    "    current_index+=len(PS_limit_ks_z)\n",
    "        \n",
    "    #z=10 21cm PS\n",
    "    PS_limit_ks_z = np.fromfile('HERA_Phase1_Limits/PS_limit_ks_z10.bin') \n",
    "    axs[1].plot(PS_limit_ks_z, ..., color=colors[ii])\n",
    "    axs[1].plot(PS_limit_ks_z, ..., color=colors[ii], ls='-.')\n",
    "    if ii < Ntest_show-1: \n",
    "        current_index=2\n",
    "    else:\n",
    "        current_index+=len(PS_limit_ks_z)\n",
    " \n",
    "    axs[0].text(0.01, 0.99, r'$z=8$',ha='left',va='top',transform=axs[0].transAxes,fontsize = 15) \n",
    "    axs[1].text(0.01, 0.99, r'$z=10$',ha='left',va='top',transform=axs[1].transAxes,fontsize = 15) \n",
    "   \n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05,wspace=0.05)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "\n",
    "plt.xlabel('\\n\\n'+r'$k[{\\rm Mpc}^{-1}]$', fontsize=15)\n",
    "plt.ylabel(r'$\\Delta_{21}^2[{\\rm mK}^{2}]$'+'\\n\\n', fontsize=15)\n",
    "\n",
    "fig, axs = plt.subplots(5,1, figsize=(5,15),sharex=True, sharey=True)\n",
    "# UV LF @ z=6\n",
    "redshift=6\n",
    "fLF = 'LFs/LF_obs_Bouwens_%.6f.txt'%redshift\n",
    "for ii in range(Ntest_show):\n",
    "    observed_muv = np.loadtxt(fLF, usecols=0)\n",
    "    observed_muv = observed_muv[observed_muv>-20]\n",
    "    axs[ii].plot(observed_muv, ..., color=colors[ii])\n",
    "    axs[ii].plot(observed_muv, ..., color=colors[ii], ls='-.')           \n",
    "    axs[ii].text(0.01, 0.99, r'$z=6$',ha='left',va='top',transform=axs[ii].transAxes,fontsize = 15) \n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.05,wspace=0.05)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "\n",
    "plt.xlabel('\\n\\n'+r'$M_{\\rm 1500}$', fontsize=15)\n",
    "plt.ylabel(r'$\\phi$'+'\\n\\n', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's check all 54D outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(9,6, figsize=(15,10), sharex=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "frac_err = ...\n",
    "for ii in range(Y_test.shape[1]):\n",
    "    axs[ii].hist(frac_err[ii], bins=np.linspace(-1,1,100), color='k')\n",
    "    axs[ii].yaxis.set_tick_params(labelsize=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.0,wspace=0.0)\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "\n",
    "plt.xlabel('\\n\\n'+r'$({Y_{pred} - Y_{true}})/{Y_{true}}$', fontsize = 30)\n",
    "plt.ylabel('Histogram', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution / thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we improve the network for parameter space where its performance is poor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# save the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
